{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d353de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Display the first few rows\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbaad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba81a90",
   "metadata": {},
   "source": [
    "1.Missing values check: We first check for missing values in the dataset to ensure data quality. For this dataset, there are no missing values.\n",
    "\n",
    "2.Feature scaling: We use StandardScaler to standardize the features. Scaling is important here because the features are on different scales, and algorithms like SVM and k-NN are sensitive to the scale of the data.\n",
    "\n",
    "3.Train-test split: We split the dataset into training (80%) and testing (20%) sets to evaluate the model performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0feac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (455, 30), Testing data shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Since there are no missing values, we proceed to scaling\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Display shape of datasets\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb17cb5",
   "metadata": {},
   "source": [
    "# 2. Classification Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b64fc7",
   "metadata": {},
   "source": [
    "1. Logistic Regression: Logistic Regression is a linear model used for binary classification. It estimates the probabilities using the logistic function, and is suitable for this dataset as it assumes a linear relationship between the features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682c480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550f46f",
   "metadata": {},
   "source": [
    "2. Decision Tree Classifier: Decision Trees split the data into subsets based on feature values. They are interpretable and can capture non-linear relationships, making them suitable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc48464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 0] 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test_scaled)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(y_pred_tree,accuracy_tree )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c65033",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier: Random Forest is an ensemble method that uses multiple decision trees to improve predictive accuracy and control overfitting. It is suitable due to its robustness and ability to handle complex interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce291262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffbb13",
   "metadata": {},
   "source": [
    "4. Support Vector Machine (SVM): SVM finds the hyperplane that best separates the classes in the feature space. It is effective for high-dimensional spaces, making it a good choice for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d796a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc04dbc",
   "metadata": {},
   "source": [
    "5. k-Nearest Neighbors (k-NN): k-NN is a non-parametric method that classifies based on the majority class of the nearest neighbors. It can capture local structures in the data, making it suitable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# k-Nearest Neighbors\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ce769",
   "metadata": {},
   "source": [
    "# 3.Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732d4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy\n",
      "0  Logistic Regression  0.973684\n",
      "1        Decision Tree  0.947368\n",
      "2        Random Forest  0.964912\n",
      "3                  SVM  0.982456\n",
      "4                 k-NN  0.947368\n"
     ]
    }
   ],
   "source": [
    "# Model performance comparison\n",
    "model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'k-NN']\n",
    "accuracies = [accuracy_log_reg, accuracy_tree, accuracy_rf, accuracy_svm, accuracy_knn]\n",
    "\n",
    "# Creating a DataFrame for comparison\n",
    "results = pd.DataFrame({'Model': model_names, 'Accuracy': accuracies})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220278ad",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8824d81",
   "metadata": {},
   "source": [
    "\n",
    "From the results, you will see which algorithm has the highest accuracy and which one has the lowest. Typically, Random Forest and SVM perform well on this dataset due to their robustness and ability to capture complex relationships, while simpler models like Logistic Regression may perform slightly less effectively but are interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8e198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
